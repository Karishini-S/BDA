# âš™ï¸ Project Setup & Execution Guide

This guide walks through the complete setup and execution steps for the **Multi-Modal Fake Review and Product Scam Detection System**, including Docker services, Scala preprocessing, HDFS integration, and Spark job execution.

---

## ğŸš€ Start Docker Services

In the project root directory:

### ğŸ”¹ Pull Required Images
```bash
docker-compose pull
```

### ğŸ”¹ Start All Containers
```bash
docker-compose up -d
```

### ğŸ”¹ Verify Active Containers
```bash
docker ps
```

You should see containers named:
- `jupyterlab`
- `datanode`
- `namenode`
- `spark-master`
- `spark-worker`

---

## ğŸ› ï¸ Compile Scala Code

After modifying or creating a new Scala preprocessing job:

### ğŸ”¹ Package the Scala Code
```bash
sbt package
```

This generates a JAR file at:
```
target/scala-2.12/amazonreviewspreprocessing_2.12-0.1.jar
```

---

## ğŸ“¦ Copy JAR into Spark Container

Transfer the compiled JAR into the Spark Master container:
```bash
docker cp ./target/scala-2.12/amazonreviewspreprocessing_2.12-0.1.jar spark-master:/opt/bitnami/spark/
```

---

## ğŸ“ Upload Dataset to HDFS

### ğŸ”¹ Create Local Directory in Namenode
```bash
docker exec -it namenode mkdir -p /data
```

### ğŸ”¹ Copy Dataset from Host to Namenode
```bash
docker cp ./data/<filename> namenode:/data/
```

### ğŸ”¹ Access Namenode Bash (Terminal 2 Recommended)
```bash
docker exec -it namenode bash
```

### ğŸ”¹ Inside Namenode Bash
```bash
hdfs dfs -mkdir -p /data
hdfs dfs -put /data/<filename> /data/
```

### ğŸ”¹ Verify Upload
```bash
hdfs dfs -ls /data
```

### ğŸ”¹ Create Output Directory for Spark
```bash
hdfs dfs -mkdir -p /output
hdfs dfs -chmod 777 /output
```

---

## ğŸ§ª Run Preprocessing Job

### ğŸ”¹ Access Spark Master Bash (Terminal 3 Recommended)
```bash
docker exec -it spark-master bash
```

### ğŸ”¹ Submit Spark Job
```bash
spark-submit \
  --class PreprocessReviews \
  /opt/bitnami/spark/amazonreviewspreprocessing_2.12-0.1.jar \
  /data/<filename> \
  hdfs://namenode:9000/output/cleaned_reviews
```

---

## ğŸ“‚ Verify Output in HDFS

In Terminal 2 (namenode bash):
```bash
hdfs dfs -ls /output/cleaned_reviews
```

---

## ğŸ” Inspect Output in Spark Shell

### ğŸ”¹ Access Spark Master Bash (Terminal 4 Recommended)
```bash
docker exec -it spark-master bash
```

### ğŸ”¹ Launch Spark Shell
```bash
spark-shell
```

### ğŸ”¹ Load and View Preprocessed Data
```scala
val df = spark.read.parquet("hdfs://namenode:9000/output/cleaned_reviews")
df.printSchema()
df.show(10)
```

---
